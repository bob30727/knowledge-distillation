import json
from datasets import Dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import torch
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

# âœ… è®€å– JSON è¨“ç·´é›†
with open("MNLI_train_6.json", "r", encoding="utf-8") as f:
    train_data = json.load(f)

# âœ… è®€å– JSON æ¸¬è©¦é›†
with open("MNLI_eval.json", "r", encoding="utf-8") as f:
    eval_data = json.load(f)

# âœ… å®šç¾©æ¨™ç±¤å°æ‡‰
label_mapping = {"entailment": 0, "contradiction": 1}

# âœ… è½‰æ›æ¨™ç±¤æ ¼å¼ï¼Œç¢ºä¿å¤§å°å¯«ä¸€è‡´
for dataset in [train_data, eval_data]:
    for item in dataset:
        if "label" in item:
            label_lower = item["label"].lower()  # è½‰æ›ç‚ºå°å¯«
            if label_lower in label_mapping:
                item["label"] = label_mapping[label_lower]
            else:
                raise ValueError(f"âŒ ç™¼ç¾ç„¡æ•ˆçš„æ¨™ç±¤å€¼: {item['label']}")

# âœ… è½‰æ›æˆ Hugging Face Dataset
dataset_train = Dataset.from_list(train_data)
dataset_eval = Dataset.from_list(eval_data)

# âœ… åŠ è¼‰ Tokenizer
model_name = "./deberta-v3-large-zeroshot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# âœ… Tokenize æ•¸æ“š
def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True, padding="max_length")

tokenized_train_dataset = dataset_train.map(preprocess_function, batched=True)
tokenized_eval_dataset = dataset_eval.map(preprocess_function, batched=True)

# âœ… é‡æ–°å‘½åæ¨™ç±¤æ¬„ä½
tokenized_train_dataset = tokenized_train_dataset.rename_column("label", "labels")
tokenized_eval_dataset = tokenized_eval_dataset.rename_column("label", "labels")
tokenized_train_dataset.set_format("torch")
tokenized_eval_dataset.set_format("torch")

# âœ… åŠ è¼‰ DeBERTa æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,
)

# âœ… è¨“ç·´åƒæ•¸
training_args = TrainingArguments(
    output_dir="./deberta_mnli_finetuned_6",
    evaluation_strategy="epoch",  # æ¯å€‹ epoch è©•ä¼°ä¸€æ¬¡
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    load_best_model_at_end=False,
    logging_dir="./logs",
    logging_steps=1,
    push_to_hub=False
)

# âœ… ç¢ºä¿æ‰¹æ¬¡è³‡æ–™èƒ½å¤ è¢«æ­£å¸¸è™•ç†
data_collator = DataCollatorWithPadding(tokenizer)

# âœ… è¨­å®š Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,  # åŠ å…¥æ¸¬è©¦é›†
    tokenizer=tokenizer,
    data_collator=data_collator
)

# âœ… é–‹å§‹å¾®èª¿
trainer.train()

# âœ… è¨“ç·´å®Œæˆå¾Œï¼ŒåŸ·è¡Œæ¸¬è©¦é›†è©•ä¼°
eval_results = trainer.evaluate()

# âœ… é¡¯ç¤ºæ¸¬è©¦çµæœ
test_loss = eval_results.get("eval_loss", "N/A")
print("ğŸ“Š æ¸¬è©¦é›†è©•ä¼°çµæœ:", eval_results)
print(f"ğŸ“‰ Test Loss: {test_loss}")

# âœ… å„²å­˜æ¨¡å‹
model.save_pretrained("./deberta_mnli_finetuned_6")
tokenizer.save_pretrained("./deberta_mnli_finetuned_6")
